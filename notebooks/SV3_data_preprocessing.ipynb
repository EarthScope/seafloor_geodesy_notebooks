{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f3f9e35",
   "metadata": {},
   "source": [
    "# SV3 Data Preprocessing \n",
    "\n",
    "This notebook runs a user through the steps to select a campaign and preprocess all the raw data into the inputs necessary to run GARPOS.  \n",
    "\n",
    "It is specific to the steps for processing SV3 data.  \n",
    "\n",
    "## Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f568c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from es_sfgtools.workflows.workflow_handler import WorkflowHandler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4636ccb6",
   "metadata": {},
   "source": [
    "### Browse available campaigns from the community archive and select target\n",
    "- Locate the campaign of interest in https://gage-data.earthscope.org/archive/seafloor, and note the `network`, `station`, and `campaign` names, which will be input in the cell below.  \n",
    "\n",
    "- Note: the cascadia-gorda raw data is currently hidden (by request) but still usable, here are the available campaigns\n",
    "\n",
    "    |  | GCC1 | NBR1 | NCC1 |\n",
    "    |---|---|---|---|\n",
    "    | **2022** |2022_A_1065 | 2022_A_1065  |  2022_A_1065 |\n",
    "    | **2023** |  2023_A_1063 | 2023_A_1063 | 2023_A_1063 |\n",
    "    | **2024** |  2024_A_1126 |  2024_A_1126 | 2024_A_1126 |\n",
    "\n",
    "\n",
    "- In order to use this notebook to process new campaigns, the data must first be submitted and made available from the community archive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f641820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input survey parameters\n",
    "network='cascadia-gorda'\n",
    "site='NCC1'\n",
    "campaign='2023_A_1063'\n",
    "\n",
    "# Set data directory path for local environment\n",
    "data_dir = Path(f\"{os.path.expanduser('~/data/sfg')}\")\n",
    "raw_data_dir = data_dir / network / site / campaign / \"raw\"\n",
    "\n",
    "\n",
    "#### USE THE FOLLOWING DEFAULTS UNLESS DESIRED ####\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "workflow = WorkflowHandler(directory=data_dir)\n",
    "workflow.set_network_station_campaign(network_id=network, \n",
    "                                      station_id=site, \n",
    "                                      campaign_id=campaign)\n",
    "                                      \n",
    "print(f\"Workflow directory: {workflow.directory}\")\n",
    "print(f\"Raw data directory for campaign: {raw_data_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fa6ec5",
   "metadata": {},
   "source": [
    "## Optional Steps - ingest raw data or download raw data from the cloud\n",
    "### Option 1: Ingest Local Raw Data\n",
    "\n",
    "If you already have raw data files downloaded on your local machine, use this option to register them with the workflow catalog.\n",
    "\n",
    "The code below will scan the `raw_data_dir` directory (defined above) and add any existing raw data files to the internal catalog, making them available for processing. This is useful when you've manually downloaded files or are reusing data from a previous session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2157241e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest raw data from the local raw data directory\n",
    "workflow.ingest_add_local_data(raw_data_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bda2b9",
   "metadata": {},
   "source": [
    "### Option 2: Download Data from Community Archive\n",
    "\n",
    "If you don't have data downloaded locally, this option retrieves raw data from the EarthScope community archive (https://gage-data.earthscope.org/archive/seafloor).\n",
    "\n",
    "**Two-step process:**\n",
    "1. **Catalog the available data**: `workflow.ingest_catalog_archive_data()` queries the archive and creates an inventory of available files for your selected campaign\n",
    "\n",
    "2. **Download the raw data**: `workflow.ingest_download_archive_data()` downloads the necessary raw data files to your local directory\n",
    "\n",
    "The workflow automatically identifies and downloads the appropriate file types. Files already present locally are skipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dbe274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ingest catalog data\n",
    "workflow.ingest_catalog_archive_data()\n",
    "\n",
    "# Download data\n",
    "workflow.ingest_download_catalog_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3dc94b3",
   "metadata": {},
   "source": [
    "## Configure Processing Parameters\n",
    "\n",
    "The `global_config` dictionary below defines settings for each stage of the data processing pipeline. These settings control how raw data is converted into GARPOS-ready inputs.\n",
    "\n",
    "### Configuration Sections:\n",
    "\n",
    "- **`dfop00_config`**: Settings for processing acoustic ping/reply sequences from DFOP00 files\n",
    "  - `override`: Set to `True` to reprocess even if data already exists\n",
    "\n",
    "- **`novatel_config`**: Settings for processing GNSS range observations from Novatel receivers\n",
    "  - `n_processes`: Number of parallel processes to use (adjust based on your CPU cores)\n",
    "  - `override`: Set to `True` to reprocess existing data\n",
    "\n",
    "- **`position_update_config`**: Settings for interpolating waveglider positions to ping times\n",
    "  - `lengthscale`: Gaussian process lengthscale parameter for smoothing (in seconds)\n",
    "  - `plot`: Set to `True` to generate diagnostic plots\n",
    "  - `override`: Set to `True` to reprocess existing data\n",
    "\n",
    "- **`pride_config`**: Settings for PRIDE-PPPAR precise point positioning\n",
    "  - `cutoff_elevation`: Minimum satellite elevation angle (degrees)\n",
    "  - `system`: GNSS constellation(s) - \"GREC23J\" = GPS/GLONASS/Galileo/BDS/QZSS\n",
    "  - `frequency`: GNSS frequency bands to use for each system\n",
    "  - `loose_edit`: Use relaxed editing for high-dynamic waveglider data\n",
    "  - `sample_frequency`: Output position sampling rate (Hz)\n",
    "  - `tides`: Tide corrections - \"SOP\" = solid/ocean/polar\n",
    "  - `override`: Set to `True` to reprocess existing solutions\n",
    "  - `override_products_download`: Set to `True` to re-download orbit/clock products\n",
    "\n",
    "- **`rinex_config`**: Settings for generating RINEX observation files\n",
    "  - `n_processes`: Number of parallel processes for RINEX generation\n",
    "  - `time_interval`: Length of each RINEX file in hours (24 = daily files)\n",
    "  - `override`: Set to `True` to regenerate existing RINEX files\n",
    "\n",
    "**Note**: Set `override=False` to skip processing steps where outputs already exist. This is useful for resuming interrupted workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c35ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_config = {\n",
    "    \"dfop00_config\": {\n",
    "        \"override\": True\n",
    "        },\n",
    "    \"novatel_config\": {\n",
    "        \"n_processes\": 14,\n",
    "        \"override\": False\n",
    "        },\n",
    "    \"position_update_config\": {\n",
    "        \"override\": True,\n",
    "        \"lengthscale\": 0.1,\n",
    "        \"plot\": False\n",
    "        },\n",
    "    \"pride_config\": {\n",
    "        \"cutoff_elevation\": 7,\n",
    "        \"end\": None,\n",
    "        \"frequency\": [\"G12\", \"R12\", \"E15\", \"C26\", \"J12\"],\n",
    "        \"high_ion\": None,\n",
    "        \"interval\": None,\n",
    "        \"local_pdp3_path\": None,\n",
    "        \"loose_edit\": True,\n",
    "        \"sample_frequency\": 1,\n",
    "        \"start\": None,\n",
    "        \"system\": \"GREC23J\",\n",
    "        \"tides\": \"SOP\",\n",
    "        \"override_products_download\": False,\n",
    "        \"override\": False\n",
    "        },\n",
    "    \"rinex_config\": {\n",
    "        \"n_processes\": 14,\n",
    "        \"time_interval\": 24,\n",
    "        \"override\": False\n",
    "        }\n",
    "    }   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052c019e",
   "metadata": {},
   "source": [
    "## Run the Complete SV3 Processing Pipeline\n",
    "\n",
    "This single command executes the entire data processing workflow, transforming raw SV3 data into GARPOS-ready observation files.\n",
    "\n",
    "**What it does:**\n",
    "1. `process_novatel`: Preprocesses Novatel 770 and 000 binary files for the current context\n",
    "    1. **Novatel 770**: Extracts GNSS observations to primary TileDB array\n",
    "    2. **Novatel 000**: Extracts GNSS observations to secondary array + IMU\n",
    "           positions\n",
    "        \n",
    "2. `build_rinex`: Generates and catalog daily RINEX files for the current campaign.\n",
    "    1. Consolidates GNSS observation data\n",
    "    2. Determines processing year from config or campaign name\n",
    "    3. Invokes tile2rinex to generate daily RINEX files\n",
    "    4. Creates AssetEntry for each RINEX file\n",
    "    5. Updates asset catalog with merge job\n",
    "\n",
    "3. `run_pride`: Runs PRIDE-PPP on RINEX files to generate KIN and residual files.\n",
    "    1. Retrieves RINEX files needing processing\n",
    "    2. Downloads GNSS product files (SP3, OBX, ATT) for each unique DOY\n",
    "    3. Runs PRIDE-PPPAR in parallel to convert RINEX to KIN format\n",
    "    4. Adds KIN and residual files to asset catalog\n",
    "\n",
    "5. `process_kinematic`: Processes KIN files to generate kinematic position dataframes.\n",
    "    1. Retrieves KIN files needing processing\n",
    "    2. Converts each KIN file to a structured dataframe\n",
    "    3. Writes dataframes to kinematic position TileDB array\n",
    "    4. Marks files as processed in asset catalog\n",
    "\n",
    "6. `process_dfop00`: Processes Sonardyne DFOP00 files to generate preliminary shotdata.\n",
    "    1. Retrieves DFOP00 files needing processing\n",
    "    2. Converts each file to shotdata dataframe (acoustic ping-reply\n",
    "        sequences)\n",
    "    3. Writes dataframes to preliminary shotdata TileDB array\n",
    "    4. Marks files as processed in asset catalog\n",
    "\n",
    "7. `update_shotdata`: Refines shotdata with interpolated high-precision kinematic positions. This step significantly improves position accuracy by replacing GNSS\n",
    "    positions with interpolated PRIDE-PPP solutions.\n",
    "    1. Gets merge signature from preliminary shotdata and kinematic\n",
    "    position arrays\n",
    "    2. Checks if refinement is needed (via override or merge status)\n",
    "    3. Merges shotdata with interpolated kinematic positions\n",
    "    4. Writes refined shotdata to final TileDB array\n",
    "    5. Records merge job in asset catalog\n",
    "\n",
    "8. `process_svp`: Processes CTD and Seabird files to generate sound velocity profiles (SVP).\n",
    "    \n",
    "**Parameters:**\n",
    "- `job='all'`: Runs all processing stages sequentially. You can also specify individual stages like `process_novatel`, `build_rinex`, `run_pride`, `process_kinematic`, `process_dfop00`, `refines_shotdata`, `process_svp`\n",
    "- `primary_config=global_config`: (optional) Uses the configuration settings defined above \n",
    "- `secondary_config`: (optional) Station-specific config overrides - uncomment and define if needed for special cases\n",
    "\n",
    "**Processing time:** This can take some time depending on campaign length. Progress will be displayed as each stage completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a312bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow.preprocess_run_pipeline_sv3(\n",
    "                job='all',\n",
    "                primary_config=global_config,\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da31d36",
   "metadata": {},
   "source": [
    "## Optional: Clean Up Raw Data Files\n",
    "\n",
    "After successful processing, large raw data files can be safely removed to free up disk space. All essential data has been normalized and stored in the TileDB arrays and intermediate products.\n",
    "\n",
    "**What will be deleted:**\n",
    "- Files with `.raw` extension (GNSS observation binaries)\n",
    "- Files with `.bin` extension (acoustic data binaries)\n",
    "\n",
    "**⚠️ Warning:** This action cannot be undone. Only run this after confirming your processing completed successfully. Raw binary files can be re-downloaded from the archive if needed later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5216514e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "# Check raw data directory and find .raw and .bin files\n",
    "if raw_data_dir.exists():\n",
    "    # Find all .raw and .bin files\n",
    "    raw_files = list(raw_data_dir.rglob('*.raw'))\n",
    "    bin_files = list(raw_data_dir.rglob('*.bin'))\n",
    "    all_files_to_delete = raw_files + bin_files\n",
    "    \n",
    "    if all_files_to_delete:\n",
    "        # Calculate total size of files to be deleted\n",
    "        total_size = sum(f.stat().st_size for f in all_files_to_delete)\n",
    "        size_gb = total_size / (1024**3)\n",
    "        \n",
    "        print(f\"Raw data directory: {raw_data_dir}\")\n",
    "        print(f\"\\nFiles to be deleted:\")\n",
    "        print(f\"  - {len(raw_files)} .raw files\")\n",
    "        print(f\"  - {len(bin_files)} .bin files\")\n",
    "        print(f\"  - Total size: {size_gb:.2f} GB\")\n",
    "        \n",
    "        print(f\"\\nDetailed list:\")\n",
    "        for f in all_files_to_delete:\n",
    "            file_size = f.stat().st_size / (1024**2)\n",
    "            print(f\"  - {f.relative_to(raw_data_dir)}: {file_size:.2f} MB\")\n",
    "        \n",
    "        # Uncomment the following lines to actually delete the files\n",
    "        # print(f\"\\n⚠️  Deleting {len(all_files_to_delete)} files ({size_gb:.2f} GB)...\")\n",
    "        # for f in all_files_to_delete:\n",
    "        #     f.unlink()\n",
    "        # print(\"✓ Raw binary files deleted successfully\")\n",
    "        \n",
    "        print(\"\\n⚠️  File deletion is COMMENTED OUT for safety.\")\n",
    "        print(\"    Uncomment the deletion lines above to proceed with cleanup.\")\n",
    "    else:\n",
    "        print(f\"No .raw or .bin files found in: {raw_data_dir}\")\n",
    "else:\n",
    "    print(f\"Raw data directory not found: {raw_data_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seafloor_geodesy_mac_dependecies",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
